# Cleaning Data & the Skies

**Background:** Your are a data analyst at an environmental company. Your task is to evaluate ozone pollution across various regions.  
You’ve obtained data from the U.S. Environmental Protection Agency (EPA), containing daily ozone measurements at monitoring stations across California. However, like many real-world datasets, it’s far from clean: there are missing values, inconsistent formats, potential duplicates, and outliers.

**The Data:** The data is a modified dataset from the U.S. Environmental Protection Agency (EPA). The data file contains the daily air quality summary statistics by monitor for the state of California for 2024. Each row contains the date and the air quality metrics per collection method and site.

**The Purpose:** Before you can provide meaningful insights, you must clean and validate the data. Only then can you analyze it to uncover trends, identify high-risk regions, and assess where policy interventions are most urgently needed. The report covers the following:
- Your EDA and data cleaning process.
- How does daily maximum 8-hour ozone concentration vary over time and regions?
- Are there any areas that consistently show high ozone concentrations? Do different methods report different ozone levels?
- Consider if urban activity (weekend vs. weekday) has any effect on ozone levels across different days.
- Bonus: plot a geospatial heatmap showing any high ozone concentrations.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

This project was part of a competition on DataCamp.
